Data Washing Machine Refactor Version 1.8
Data/Time 20221028_14_39

>> Starting DWM20
Input Reference File Name = data/S1G.txt
Tokenized Reference Output File Name = logs/data/S1G-Tokenized.txt
Input File has Header Records = True
Input File Delimiter = ,
Tokenizer Function Type = Splitter
Remove Duplicate Reference Tokens = False
Total References Read= 50
Total Tokens Found = 626
Total Unique Tokens = 252
Minimum Token Frequency = 1
Maximum Token Frequency = 47
  Token= NC Frequency= 47
  Token= AARON Frequency= 33
  Token= WINSTON Frequency= 31
  Token= SALEM Frequency= 31
  Token= RD Frequency= 13
Average Token Frequency = 5.921725239616613
Standard Deviation of Token Frequency = 9.024032922506851

>>Starting DWM30
Total References Read from  Tokenized.txt = 0

>>Starting Iterations
Mu start value= 0.01
Mu iterate value= 0.05
Epsilon start value= 0.5
Epsilon iterate value= 0.0
Comparator = ScoringMatrixStd

>>Starting DWM40
Beta = 6
Min blocking token length = 0
Exclude numeric blocking tokens = False
Remove excluded blocking tokens = False
Sigma = 7
Stop Words excluded= 0
Total Blocking Records Created 0
--Ending because blockList is empty

>>Starting DWM50
Total Blocks Processed = 0
Total Pairs in Compare Cache = 0

>>Starting DWM70
Total Pairs Linked = 0  at mu= 0.01
Ending because pairList is empty
