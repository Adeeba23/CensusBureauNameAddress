Data Washing Machine Refactor Version 1.8
Data/Time 20221028_14_38

>> Starting DWM20
Input Reference File Name = data/S1G.txt
Tokenized Reference Output File Name = logs/data/S1G-Tokenized.txt
Input File has Header Records = True
Input File Delimiter = ,
Tokenizer Function Type = Splitter
Remove Duplicate Reference Tokens = False
Total References Read= 50
Total Tokens Found = 626
Total Unique Tokens = 252
Minimum Token Frequency = 1
Maximum Token Frequency = 47
  Token= NC Frequency= 47
  Token= AARON Frequency= 33
  Token= WINSTON Frequency= 31
  Token= SALEM Frequency= 31
  Token= RD Frequency= 13
Average Token Frequency = 5.921725239616613
Standard Deviation of Token Frequency = 9.024032922506851
